{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test Driven Development with unittest</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 'unittest' from the standard library. It's very capable, but you have to write classes for everything you want to test. If you want something that feels a bit more pythonic you can look at pytest. But for this tutorial we will continue with the unittest package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Iteration 1</h2>\n",
    "\n",
    "First write a test for the functionality we want our Dog class to exhibit. If Dog is finished, then this test will pass.\n",
    "\n",
    "(n.b. normally the last line here would be simply \"unittest.main()\" but I have to sprinkle some magic to make this work in jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: test_speaks (__main__.TestDog)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-7f9565b3db7e>\", line 5, in test_speaks\n",
      "    self.assertEqual(\"Woof!\", my_dog.speak())\n",
      "AttributeError: 'Dog' object has no attribute 'speak'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.005s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "class TestDog(unittest.TestCase):\n",
    "    \n",
    "    def test_speaks(self):\n",
    "        my_dog = Dog()\n",
    "        self.assertEqual(\"Woof!\", my_dog.speak())\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['dummy_for_jupyter'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully not unexpected. Now implement the method and run the test again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def speak(self):\n",
    "        return \"Woof!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1f8c82e7358>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['dummy_for_jupyter'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks okay?\n",
    "\n",
    "Now, at this point we would normally do refactoring before moving on. But there is not much here to refactor so lets just keep going.\n",
    "\n",
    "<h2>Iteration 2</h2>\n",
    "\n",
    "Add another test for the next piece of functionality we will implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDog(unittest.TestCase):\n",
    "    \n",
    "    def test_speaks(self):\n",
    "        my_dog = Dog()\n",
    "        self.assertEqual(\"Woof!\", my_dog.speak())\n",
    "        \n",
    "    def test_get_hot(self):\n",
    "        my_dog = Dog()\n",
    "        my_dog.get_hot()\n",
    "        self.assertEqual(1, my_dog.hotness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E.\n",
      "======================================================================\n",
      "ERROR: test_get_hot (__main__.TestDog)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-d4f2e7e83312>\", line 9, in test_get_hot\n",
      "    my_dog.get_hot()\n",
      "AttributeError: 'Dog' object has no attribute 'get_hot'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1f8c82e4a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['dummy_for_jupyter'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the missing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog(object):\n",
    "    def __init__(self):\n",
    "        self.hotness = 0\n",
    "    \n",
    "    def speak(self):\n",
    "        return \"Woof!\"\n",
    "    \n",
    "    def get_hot(self):\n",
    "        self.hotness = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the test should pass, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1f8c82f4eb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['dummy_for_jupyter'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually can refactor our unit tests a little bit now, using the setUp method which is called at the beginning of each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestDog(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.dog = Dog()\n",
    "    \n",
    "    def test_speaks(self):\n",
    "        self.assertEqual(\"Woof!\", self.dog.speak())\n",
    "        \n",
    "    def test_get_hot(self):\n",
    "        self.dog.get_hot()\n",
    "        self.assertEqual(1, self.dog.hotness)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['dummy_for_jupyter'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Iteration 3</h2>\n",
    "\n",
    "Now I'll check that the dog pants when it gets hot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".F.\n",
      "======================================================================\n",
      "FAIL: test_pant_when_hot (__main__.TestDog)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-07f5f6ad8332>\", line 14, in test_pant_when_hot\n",
      "    self.assertEqual(\"Pant!\", self.dog.speak())\n",
      "AssertionError: 'Pant!' != 'Woof!'\n",
      "- Pant!\n",
      "+ Woof!\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.004s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "class TestDog(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.dog = Dog()\n",
    "    \n",
    "    def test_speaks(self):\n",
    "        self.assertEqual(\"Woof!\", self.dog.speak())\n",
    "        \n",
    "    def test_get_hot(self):\n",
    "        self.dog.get_hot()\n",
    "        self.assertEqual(1, self.dog.hotness)\n",
    "        \n",
    "    def test_pant_when_hot(self):\n",
    "        self.dog.get_hot()\n",
    "        self.assertEqual(\"Pant!\", self.dog.speak())        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['dummy_for_jupyter'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll just go ahead and fix up the Dog..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog(object):\n",
    "    def __init__(self):\n",
    "        self.hotness = 0\n",
    "    \n",
    "    def speak(self):\n",
    "        if self.hotness > 0:\n",
    "            return \"Pant!\"\n",
    "        return \"Woof!\"\n",
    "    \n",
    "    def get_hot(self):\n",
    "        self.hotness = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...so our tests should all pass now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1f8c8303320>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['dummy_for_jupyter'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
